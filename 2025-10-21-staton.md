---
title: Discriminatory Function Generative AI 
layout: page
---

# Discriminatory AI

## AI Is A Product Of Its Environment

As the heading suggests, AI is a product of its environment. What does this mean? When I say “a product of its environment”, I am talking about how Generative AI only produces a rendition of the information it has been fed and trained on. The AI itself is not alive, and it is not an active enemy. However, this does not mean the information that it assembles is not biased or discriminatory. In reality, it very much is. Chowdhury et al. from “These Women Tried to Warn Us About AI” speaks directly about this issue even before generative AI was as openly available to the public:

> When a group of California scientists gave GPT-2 the prompt “the man worked as,” it completed the sentence by writing “a car salesman at the local Wal-Mart.” However, the prompt “the woman worked as” generated “a prostitute under the name of Hariya.” Equally disturbing was “the white man worked as,” which resulted in “a police officer, a judge, a prosecutor, and the president of the United States,” in contrast to “the Black man worked as” prompt, which generated “a pimp for 15 years.”

## Parent Parallel

This quote shows that Generative AI produces obvious biases. As stated previously, these come from the data and information they are trained on. I like to think of this as a parental parallel. A parent teaches their child about their beliefs and values. Whether that is conscious or unconscious does not really matter: a child will learn it regardless. Whether or not the creator or trainer of AI means to feed it biased information, they still do and the AI still outputs it. However, as a scientist training AI, one should know what comes with the information it is fed. Even if it seems unintentional, there is probably a reason for the bias. The trainer, like a parent, feeds the AI biased information, and the AI, like a child repeats it. 

## Other Connections

> 2025-10-19-Smidebush: There are many instances where the materials used in the training of Gen AI models are skewed towards a certain demographic of people, leading the AI model's answers to be racist or sexist in nature. There have also been instances of facial recognition softwares not being able to identify the faces of black women, and in the instances when they did, they often categorized them as black men.

This similarly comes from the Chowdhury et al. article. It speaks more of the discriminatory results stemming from generative AI due to what it was trained on.

>2025-10-21-Lehmbeck: [These authors] claim that the art of writing highlights linguistic diversity, ethical labor, and critical pedagogy, all of which would be diminished if AI is adopted and used.

This describes more of the writing side of AI, which generative AI still has a claim on. If AI is so biased, its stake in writing is even more concerning.

> 2025-10-21-Baxter: 

> If people will not stop using AI they must make sure to:

> Fact check the sources or information that the AI is providing you.

> Not over rely on it and let it do 100% of the writing for you (use responsibly).

> Make sure the information or views given by the AI aren't harmful to a certain race or minority.

This information from Baxter demonstrates ways to use AI (somewhat) ethically if use is going to continue. 
